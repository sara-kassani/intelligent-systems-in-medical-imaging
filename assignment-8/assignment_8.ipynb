{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teaching Assistants:\n",
    "- David Tellez: david.tellezmartin@radboudumc.nl\n",
    "- Wouter Bulten: wouter.bulten@radboudumc.nl\n",
    "\n",
    "Please submit your notebook via grand-challenge.org (https://ismi-amida13.grand-challenge.org/).\n",
    "Submit a notebook **WITH ALL CELLS EXECUTED!!!**\n",
    "\n",
    "* Groups: You should work in pairs or alone\n",
    "* Deadline for this assignment: \n",
    " * Friday (April 27th) until midnight\n",
    " * 5 points (maximum grade = 100 points) penalization per day after deadline\n",
    "* Submit your **fully executed** notebook to the grand-challenge.org platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your name\n",
    "Write the name(s) of the author(s) of this assignment below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Brigel Pineti, s1005549, b.pineti@student.ru.nl \n",
    "* Christoph Schmidl, s4226887, c.schmidl@student.ru.nl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection of Mitotic Figures in H&E Stained Breast Cancer Tissue Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Clinical Problem\n",
    "\n",
    "Computational Pathology is the branch of Medical Image Analysis that deals with images of Anatomical Pathology, essentially **human cells and tissue structures at very high magnification**. Typically, a piece of human tissue is cut into very thin slices, stained and examined under the microscope by human pathologists. Lately, these slices are digitazed using so-called whole-slide scanners:\n",
    "\n",
    "<img src=\"./figures/computational_pathology.png\" align=\"center\" width=\"700\">\n",
    "\n",
    "These digitized slides are called \"whole-slide images\" (WSIs) and are gigapixel images (typically RGB 100000x100000 pixels):\n",
    "\n",
    "<img src=\"./figures/wsi_patches.png\" align=\"center\" width=\"700\">\n",
    "\n",
    "### The Task\n",
    "\n",
    "Your task is to **build a convolutional neural network that can effectively detect a special structure: mitotic figures**. These are cells that are dividing ([mitosis](https://en.wikipedia.org/wiki/Mitosis)) within a tumor in the breast. Pathologists are interested in detecting and counting these figures because it has been shown that the number of mitotic figures per tissue area is strongly correlated with patient death (survival). In other words, the more mitotic figures per area found in a given tumor sample, the more likely that the patient will die. The hypothesis is that aggressive tumors have more dividing cells than less aggressive ones:\n",
    "\n",
    "<img src=\"./figures/mitosis.png\" align=\"center\" width=\"550\">\n",
    "\n",
    "### The Data\n",
    "\n",
    "To simplify the task, your network will only need to **classify small tissue patches**, instead of entire whole-slide images. We have prepared these patches for you (training and test). Several pathologists have annotated mitotic figures in tissue tiles and we have extracted both positive (mitotic) and negative (non-mitotic) tissue patches from a publicly available dataset. You can see some examples of mitotic figures below (green arrows):\n",
    "\n",
    "<img src=\"./figures/amida.png\" align=\"center\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This assignment\n",
    "\n",
    "This assignment consist of 8 subtasks with a total of 100 points:\n",
    "\n",
    "### Task 1. Data Preprocessing: 10 points\n",
    "You will explore the data (2.5p), parse filenames (2.5p), create a Pandas dataframe (2.5p), and split the dataset into training and validation (2.5p).\n",
    "\n",
    "### Task 2. Data Handlers: 15 points\n",
    "You will create a Keras-compatible mini-batch generator (5p), a Keras-compatible mini-batch sequence (5p), and add support for data augmentation (5p). \n",
    "\n",
    "### Task 3. Network Training: 10 points\n",
    "You will design the network architecture (5p) and train the network with fit_generator (5p).\n",
    "\n",
    "### Task 4. Training Callbacks: 10 points\n",
    "You will include Keras callbacks during training (5p), and design a custom plot callback (5p). \n",
    "\n",
    "### Task 5. Evaluation: 25 points\n",
    "You will define the F1-score function (5p), plot the F1-score curve (5p), and create a custom callback to evaluate the validation set at the end of training and find the best detection threshold (10p). Additionally, you will create another custom callback to make predictions on the test set and prepare a submission file (5p).\n",
    "\n",
    "### Task 6. Train a classifier: 10 points\n",
    "You will create a function to train a model and submit the results (10p).\n",
    "\n",
    "### Task 7. Hard negative mining: 10 points\n",
    "You will obtain scores for the training set (5p), and resample the training set (5p). \n",
    "\n",
    "### Task 8. Retrain the classifier: 10 points\n",
    "You will train the model with the hard dataset and submit the results (10p).\n",
    "\n",
    "---\n",
    "\n",
    "As done in previous assignments, in this notebook we provide some parts of code implemented.\n",
    "Some other parts are not implemented, but we define the variables that will be used in functions, to help you in the development of the assignment.\n",
    "Things that have been declared but not implemented are assigned a **None** value.\n",
    "That is the part that you have to implement.\n",
    "This means that every time you see **None**, it means that something is missing and you have to implement it.\n",
    "\n",
    "## Reporting your results\n",
    "\n",
    "When you are done with the assignments you will have to hand in this notebook. Make sure that for each assignment you output a summary of the network architecture and a plot of the loss/accuracy curve during training. This output will be used to grade your assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# System\n",
    "from os.path import join, basename, dirname, exists  \n",
    "import os  \n",
    "from glob import glob \n",
    "import random \n",
    "from tqdm import tqdm\n",
    "\n",
    "# Computational\n",
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "from matplotlib import pyplot as plt  \n",
    "\n",
    "# Deep learning\n",
    "import keras\n",
    "\n",
    "# Other\n",
    "from scipy.ndimage import imread  \n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If the Jupter server runs on Cartesius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/projects/0/ismi2018/AMIDA13'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If the Jupyter server runs on your computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm_notebook\n",
    "import zipfile\n",
    "\n",
    "link = 'https://surfdrive.surf.nl/files/index.php/s/1acVhUNggKyFLDk/download'\n",
    "file_name = \"ismi-amida13.zip\"\n",
    "with open(file_name, \"wb\") as f:\n",
    "        response = requests.get(link, stream=True)\n",
    "        total_length = response.headers.get('content-length')\n",
    "        if total_length is None: # no content length header\n",
    "            f.write(response.content)\n",
    "        else:\n",
    "            dl = 0\n",
    "            total_length = int(total_length)\n",
    "            for data in tqdm_notebook(response.iter_content(chunk_size=4096), desc='Downloading data'):\n",
    "                dl += len(data)\n",
    "                f.write(data)\n",
    "with zipfile.ZipFile(file_name,\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./\")\n",
    "os.remove('./ismi-amida13.zip')\n",
    "data_dir = './' # define here the directory where you have your data, downloaded from SURFDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create result dir\n",
    "result_dir = './results' # define here the directory where your results will be saved\n",
    "if not exists(result_dir):\n",
    "    os.mkdir(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Data Preprocessing: 10 points\n",
    "You will explore the data (2.5p), parse filenames (2.5p), create a Pandas dataframe (2.5p), and split the dataset into training and validation (2.5p).\n",
    "\n",
    "Data preprocessing is typically the task that takes most of the development time when building machine learning applications. In this section, you will learn how to manage your data using the same tools that advanced research labs use. In particular, you will learn the basics of [Pandas](https://pandas.pydata.org/) dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Data (5p)\n",
    "\n",
    "Investigate the input data: list paths to files, load images, plot them, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Collect paths for all images (positive and negative training, and test images)\n",
    "training_positives_paths = glob(join(data_dir, 'training', 'positive', '*.png'))\n",
    "training_negatives_paths = glob(join(data_dir, 'training', 'negative', '*.png'))\n",
    "test_paths = glob(join(data_dir, 'test', '*.png'))\n",
    "\n",
    "print('Sample positive training path: {path}'.format(path=training_positives_paths[0]))\n",
    "print('Sample negative training path: {path}'.format(path=training_negatives_paths[0]))\n",
    "print('Sample test path: {path}'.format(path=test_paths[0]))\n",
    "\n",
    "print('There are {a} mitotic and {b} non-mitotic training patches'.format(a=len(training_positives_paths), b=len(training_negatives_paths)))\n",
    "print('There are {a} test patches'.format(a=len(test_paths)))\n",
    "\n",
    "# Define a function to read images from disk and convert them to xyc format in a desire output range.\n",
    "def load_image(input_path, range_min=0, range_max=1):\n",
    "    \n",
    "    # Read image data (x, y, c) [0, 255]\n",
    "    image = imread(input_path)\n",
    "    \n",
    "    # Convert image to the correct range\n",
    "    image = None\n",
    "\n",
    "    return image\n",
    "\n",
    "# Define a function to plot a batch or list of image patches in a grid\n",
    "def plot_image(images, images_per_row=8):\n",
    "    \n",
    "    fig, axs = plt.subplots(int(np.ceil(len(images)/images_per_row)), images_per_row)\n",
    "    \n",
    "    c = 0\n",
    "    for ax_row in axs:\n",
    "        for ax in ax_row:\n",
    "            if c < len(images):\n",
    "                ax.imshow(images[c])\n",
    "            ax.axis('off')            \n",
    "            c += 1\n",
    "    plt.show()\n",
    "\n",
    "# Read a few example images and plot them\n",
    "images_positive = [load_image(path) for path in training_positives_paths[:32]]\n",
    "images_negative = [load_image(path) for path in training_negatives_paths[:32]]\n",
    "\n",
    "print('Some positives examples (shape {shape}):'.format(shape=images_positive[0].shape))\n",
    "plot_image(images_positive, images_per_row=8)\n",
    "\n",
    "print('Some negative examples (shape {shape}):'.format(shape=images_negative[0].shape))\n",
    "plot_image(images_negative, images_per_row=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Filenames (2.5p)\n",
    "\n",
    "Notice that the file names contain valuable metadata. For example '0.000655216630548_08_06_Normalized_1635_1047_1.png' reflects the following:\n",
    "* Score (ignore for this assignment): 0.000655216630548\n",
    "* Patient case: 08\n",
    "* Tile ID: 06\n",
    "* Stain standardization status: Normalized\n",
    "* X-location: 1635\n",
    "* Y-location: 1047\n",
    "* Label (mitotic 1, non-mitotic 0): 1\n",
    "\n",
    "As in many machine learning problems, plenty of metadata is not relevant to the task at hand. For this problem, you will only need the **patient case** and **label**. You will write a function that extracts this information from a given path. Please notice that **test patches do not have score or label**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract meta-data from a sample patch path in a dictionary format.\n",
    "def parse_file_name(file_path, test=False):\n",
    "    \n",
    "    # Training sample: 0.9620013237_02_27_Normalized_1175_78_1.png\n",
    "    # Test sample: 01_01_Normalized_1001_856.png\n",
    "    filename = basename(file_path)\n",
    "    \n",
    "    if not test:\n",
    "        score, case, tile, _, x, y, label = None\n",
    "    else:\n",
    "        case, tile, _, x, y = None\n",
    "        score = np.nan\n",
    "        label = -1\n",
    "        \n",
    "    data = {\n",
    "#         'score': float(score),\n",
    "        'case': int(case),\n",
    "#         'tile': int(tile),\n",
    "#         'x': int(x),\n",
    "#         'y': int(y),\n",
    "        'label': int(label),\n",
    "        'file_id': filename\n",
    "    }\n",
    "    \n",
    "    return data\n",
    "\n",
    "info1 = parse_file_name(training_positives_paths[0], test=False)\n",
    "print('The following information is available for patch path {path}:'.format(path=training_positives_paths[0]))\n",
    "print(info1)\n",
    "\n",
    "info2 = parse_file_name(test_paths[0], test=True)\n",
    "print('The following information is available for patch path {path}:'.format(path=test_paths[0]))\n",
    "print(info2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Pandas Dataframe (2.5p)\n",
    "\n",
    "Our data consists of two main folders, training and test, that contain thousands of RGB 101x101 pixel image patches. We need to divide the training set into actual training and validation sets, so that we can control overfitting during training. We will manage these sets with Pandas' dataframe structures. A dataframe is basically a table where each row is a sample and each column is an attribute (label, case, filename, etc.). Pandas provides extremely convenient functions to work with this kind of data. You can convert a list of dictionaries to a dataframe very easily. Tutorials on pandas can be found here https://pandas.pydata.org/pandas-docs/stable/tutorials.html\n",
    "\n",
    "The code below gives a small overview of basic functions of a Pandas' data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "data_dict = [\n",
    "    {'a': 1, 'b': 1, 'c': 2},\n",
    "    {'a': 0, 'b': 4, 'c': 1},    \n",
    "    {'a': 0, 'b': 4}\n",
    "]\n",
    "\n",
    "print('My dataframe looks like:')\n",
    "df = pd.DataFrame(data_dict)\n",
    "print(df)\n",
    "\n",
    "print('Columns or rows can be selected with loc[] and iloc[]:')\n",
    "print('Col \"b\":')\n",
    "col = df.loc[:, 'b']  # \":\" means \"select all items\"\n",
    "print(col)\n",
    "print('Row 1:')\n",
    "row = df.iloc[1, :]\n",
    "print(row)\n",
    "\n",
    "print('Conditional selection can be also done with loc[]:')\n",
    "print('Show columns \"a\" and \"b\" of elements where column \"b\" is 4:')\n",
    "sel = df.loc[df['b']==4, ['a', 'b']]\n",
    "print(sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get lists of image paths (we use \"all\" to refer to training+validation data)\n",
    "all_paths = glob(join(data_dir, 'training', '*', '*.png'))\n",
    "test_paths = glob(join(data_dir, 'test', '*.png'))\n",
    "\n",
    "# Parse file names into dictionaries\n",
    "all_data_dicts = [None for path in all_paths]\n",
    "test_data_dicts = [None for path in test_paths]\n",
    "\n",
    "# Convert list of dictionaries to Pandas dataframe\n",
    "df_all_data = pd.DataFrame(None)\n",
    "df_test_data = pd.DataFrame(None)\n",
    "\n",
    "\n",
    "# Explore dataframe\n",
    "print('There are {a} mitotic and {b} non-mitotic training patches'.format(\n",
    "    a=len(df_all_data.loc[df_all_data['label']==1, :]), \n",
    "    b=len(df_all_data.loc[df_all_data['label']==0, :])\n",
    "))\n",
    "print('There are {a} test patches'.format(a=len(df_test_data)))\n",
    "df_all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Training, Validation and Test (2.5p)\n",
    "\n",
    "We will use Pandas functionality to divide the dataset into training, validation and test according to patient case, and store the resulting dataframes into disk. Typically, dataframes are stored in CSV format. Use patient cases [4, 5, 7, 8, 12] cases for training and [2, 3, 9] for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set patient case\n",
    "training_cases = [8, 4, 12, 5, 7] \n",
    "validation_cases = [9, 3, 2] \n",
    "\n",
    "# Shuffle all data\n",
    "df_all_data = df_all_data.sample(len(df_all_data), replace=False).reset_index(drop=True)\n",
    "\n",
    "# Select rows whose 'case' column is among the training list\n",
    "idx_rows_within_training = df_all_data['case'].isin(training_cases)\n",
    "# Create a dataframe that consists of only the training cases (Hint: can be done with a single line using Pandas)\n",
    "df_training_data = None\n",
    "\n",
    "# Select rows whose 'case' column is among the validation list\n",
    "idxs_rows_within_validation = df_all_data['case'].isin(validation_cases)\n",
    "# Create a dataframe that consists of only the validation cases (Hint: can be done with a single line using Pandas)\n",
    "df_validation_data = None\n",
    "\n",
    "# Store file names into disk\n",
    "df_training_data.to_csv(join(result_dir, 'training_data.csv'))\n",
    "df_validation_data.to_csv(join(result_dir, 'validation_data.csv'))\n",
    "df_test_data.to_csv(join(result_dir, 'test_data.csv'))\n",
    "\n",
    "# Load CSV data from disk (to check that saving works)\n",
    "df_training_data = pd.DataFrame.from_csv(join(result_dir, 'training_data.csv'), header=0, index_col=0)\n",
    "df_validation_data = pd.DataFrame.from_csv(join(result_dir, 'validation_data.csv'), header=0, index_col=0)\n",
    "df_test_data = pd.DataFrame.from_csv(join(result_dir, 'test_data.csv'), header=0, index_col=0)\n",
    "\n",
    "# Verify that training dataframe contains training cases only (same for validation and test dataframes)\n",
    "unique_cases_in_training = df_training_data['case'].unique()\n",
    "print('Cases present in the training dataframe: {cases}'.format(cases=unique_cases_in_training))\n",
    "unique_cases_in_validation = df_validation_data['case'].unique()\n",
    "print('Cases present in the validation dataframe: {cases}'.format(cases=unique_cases_in_validation))\n",
    "unique_cases_in_test = df_test_data['case'].unique()\n",
    "print('Cases present in the test dataframe: {cases}'.format(cases=unique_cases_in_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Data Handlers: 15 points \n",
    "\n",
    "In this task you will get familiar with Keras [generators](https://keras.io/models/model/#fit_generator). This are structures that will help you manage data during training, encapsulating certain data functionality and isolating it from the rest of the code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator (5p)\n",
    "\n",
    "A generator will provide us with ready-to-use training batches. In Python, a [generator](https://wiki.python.org/moin/Generators) is an iterable object that implements the next() method. **Given a data directory and CSV path**, this object will encapsulate all the complexity of managing training data: reading patches from disk, augmentation, batch ensembling, formatting, etc. We will later use this object with Keras to feed CNNs during training. Our generator must return a new random batch on every call to `next()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PatchGenerator(object):\n",
    "\n",
    "    def __init__(self, input_dir, csv_path, batch_size, augmentation_fn=None):\n",
    "\n",
    "        # Params\n",
    "        self.input_dir = input_dir  # path to patches in glob format\n",
    "        self.csv_path = csv_path  # path to CSV with data split\n",
    "        self.batch_size = batch_size  # number of patches per batch\n",
    "        self.augmentation_fn = augmentation_fn  # augmentation function\n",
    "        \n",
    "        # Read CSV\n",
    "        self.df = pd.DataFrame.from_csv(csv_path)\n",
    "        \n",
    "        # Info\n",
    "        self.n_samples = len(self.df)\n",
    "        self.n_batches = self.n_samples // self.batch_size\n",
    "        \n",
    "        # Print some info\n",
    "        print('PatchGenerator detected: {n_samples} patch samples.'.format(n_samples=self.n_samples))\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Provide length in number of batches\n",
    "        return self.n_batches\n",
    "\n",
    "    def next(self):\n",
    "        # Build a mini-batch\n",
    "\n",
    "        # Randomly sample positive samples from the dataframe \n",
    "        # Tip: self.df.loc[self.df['label'] == 1, :].sample(...)\n",
    "        df_positives = None\n",
    "        \n",
    "        # Randomly sample negative samples from the dataframe\n",
    "        df_negatives = None\n",
    "        \n",
    "        # Concatenate positive and negative sample dataframes\n",
    "        df_batch = pd.concat([df_positives, df_negatives])\n",
    "\n",
    "        # Iterate over selected images \n",
    "        images = []\n",
    "        labels = []\n",
    "        for index, row in df_batch.iterrows():\n",
    "            \n",
    "            try:\n",
    "\n",
    "                # Read image path\n",
    "                image_path = glob(join(self.input_dir, row['file_id']))\n",
    "                assert len(image_path) == 1\n",
    "                image_path = image_path[0]\n",
    "\n",
    "                # Read data and label\n",
    "                image = None\n",
    "                label = row['label']\n",
    "\n",
    "                # Data augmentation\n",
    "                if self.augmentation_fn:\n",
    "                    image = self.augmentation_fn(image)\n",
    "\n",
    "                # Append\n",
    "                images.append(image)\n",
    "                labels.append((1-label, label))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print('Failed reading idx {idx}...'.format(idx=index))\n",
    "\n",
    "        # Assemble batch            \n",
    "        batch_x = np.stack(images).astype('float32')\n",
    "        batch_y = np.stack(labels).astype('float32')\n",
    "\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "# Test the data generator\n",
    "training_gen = PatchGenerator(\n",
    "    input_dir=join(data_dir, 'training', '*'), \n",
    "    csv_path=join(result_dir, 'training_data.csv'), \n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "for batch_x, batch_y in training_gen:\n",
    "    print(batch_x.shape)\n",
    "    print(batch_y.shape)\n",
    "    plot_image(batch_x, images_per_row=8)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sequence (5p)\n",
    "\n",
    "A Keras Sequence is very similar to a generator but designed for validation data. The main difference is that **sequences read the data in a deterministic way: always the same data in the same order** (vs. generators that read the patches randomly). Other minor differences: there is no augmentation and mini-batches are not balanced. \n",
    "\n",
    "A sequence is like a list, accessed via indexes. This means that Keras training will iterate over the length of the sequence (that you provide). Since the number of patches might not be evenly divisible by the number of samples per batch, the last mini-batch might be smaller that batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PatchSequence(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, input_dir, csv_path, batch_size):\n",
    "\n",
    "        # Params\n",
    "        self.input_dir = input_dir  # path to patches in glob format\n",
    "        self.csv_path = csv_path  # path to CSV with data split\n",
    "        self.batch_size = batch_size  # number of patches per batch\n",
    "        \n",
    "        # Read CSV\n",
    "        self.df = pd.DataFrame.from_csv(csv_path)\n",
    "        \n",
    "        # Length\n",
    "        self.n_samples = len(self.df)\n",
    "        self.n_batches = int(np.ceil(self.n_samples / self.batch_size))  # last mini-batch might be shorter\n",
    "        \n",
    "        # Print some info\n",
    "        print('PatchSequence detected: {n_samples} patch samples.'.format(n_samples=len(self.df)))\n",
    "\n",
    "    def __len__(self):\n",
    "        # Provide length in number of batches\n",
    "        return self.n_batches\n",
    "    \n",
    "    def get_all_labels(self):\n",
    "        return self.df.loc[:, 'label'].values\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # idx indexes batches, not samples\n",
    "\n",
    "        # Provide batches of samples\n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        # Create indexes for samples\n",
    "        idx1 = idx * self.batch_size\n",
    "        idx2 = np.min([idx1 + self.batch_size, self.n_samples])\n",
    "        idxs = np.arange(idx1, idx2)\n",
    "        \n",
    "        # Iterate over samples\n",
    "        for i in idxs:\n",
    "\n",
    "            try:\n",
    "                # Read image path\n",
    "                row = self.df.iloc[i, :]\n",
    "                image_path = None\n",
    "\n",
    "                # Read data and label\n",
    "                image = None\n",
    "                label = None \n",
    "\n",
    "                # Append\n",
    "                images.append(image)\n",
    "                labels.append((1-label, label))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print('Failed reading idx {idx}...'.format(idx=i))\n",
    "\n",
    "        # Assemble batch            \n",
    "        batch_x = np.stack(images).astype('float32')\n",
    "        batch_y = np.stack(labels).astype('float32')\n",
    "\n",
    "        return batch_x, batch_y\n",
    "\n",
    "# Test the sequence\n",
    "validation_seq = PatchSequence(\n",
    "    input_dir=join(data_dir, 'training', '*'), \n",
    "    csv_path=join(result_dir, 'validation_data.csv'), \n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "for batch_x, batch_y in validation_seq:\n",
    "    print(batch_x.shape)\n",
    "    print(batch_y.shape)\n",
    "    plot_image(batch_x, images_per_row=8)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation (5p)\n",
    "\n",
    "Add support for basic data augmentation (vertical and horizontal mirroring, and 90-degree rotation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_augmentation(image_array):\n",
    "    \n",
    "    # Vertical mirroring\n",
    "    if np.random.rand() > 0.5:\n",
    "        image_array = None\n",
    "\n",
    "    # Horizontal mirroring\n",
    "    if np.random.rand() > 0.5:\n",
    "        image_array = None\n",
    "\n",
    "    # 90-degree rotation\n",
    "    image_array = None\n",
    "    \n",
    "    return image_array\n",
    "\n",
    "# Test basic augmentation\n",
    "training_gen = PatchGenerator(\n",
    "    input_dir=join(data_dir, 'training', '*'), \n",
    "    csv_path=join(result_dir, 'training_data.csv'), \n",
    "    batch_size=32,\n",
    "    augmentation_fn=basic_augmentation\n",
    ")\n",
    "\n",
    "for batch_x, batch_y in training_gen:\n",
    "    print(batch_x.shape)\n",
    "    print(batch_y.shape)\n",
    "    plot_image(batch_x, images_per_row=8)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Network Training: 10 points\n",
    "\n",
    "In this task, you will define the network architecture and get familiar with fit_generator()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture (5p)\n",
    "\n",
    "Define a model for binary classification compatible with the data and labels provided before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "\n",
    "    tensor_input = None\n",
    "    tensor = None\n",
    "\n",
    "    model = keras.models.Model(inputs=tensor_input, outputs=tensor)\n",
    "    model.compile(None)\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (5p)\n",
    "\n",
    "You will use Keras fit_generator function to train the model using the data generators that we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training data\n",
    "training_data = PatchGenerator(\n",
    "    input_dir=join(data_dir, 'training', '*'), \n",
    "    csv_path=join(result_dir, 'training_data.csv'), \n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Validation data\n",
    "validation_data = PatchSequence(\n",
    "    input_dir=join(data_dir, 'training', '*'), \n",
    "    csv_path=join(result_dir, 'validation_data.csv'), \n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = model.fit_generator(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Training Callbacks: 10 points\n",
    "\n",
    "Keras offers an excellent mechanism to extend the functionality of fit_generator: [callback functions](https://keras.io/callbacks/). These callbacks allow you to save model checkpoints, reduce the learning rate during training, and other things. They are call at given events during training (beginning, end, after epoch, etc.). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic callbacks (5p)\n",
    "\n",
    "Explore the [callback functions](https://keras.io/callbacks/) and find out how to: a) reduce the learning rate when the validation loss plateaus, b) stop training if validation loss does not improve after 5 epochs, and c) save the best model weights according to the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define simple callbacks\n",
    "network_path = join(result_dir, 'best_model.h5')\n",
    "callbacks_list = [\n",
    "    None\n",
    "]\n",
    "\n",
    "# Train model\n",
    "history = model.fit_generator(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Callback (5p)\n",
    "\n",
    "Callbacks can be customized. In this task, you are required to design one that generates a plot of the loss and validation loss during training after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PlotLosses(keras.callbacks.Callback):\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.fig = plt.figure()\n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(None)\n",
    "        self.val_losses.append(None)\n",
    "        self.i += 1\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.x, self.losses, label=\"loss\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show();\n",
    "\n",
    "# Append to list of callbacks\n",
    "callbacks_list = [PlotLosses()]\n",
    "\n",
    "# Train model\n",
    "history = model.fit_generator(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5. Evaluation: 25 points\n",
    "\n",
    "You will define the F1-score function (5p), plot the F1-score curve (5p), and create a custom callback to evaluate the validation set at the end of training and find the best detection threshold (10p). Additionally, you will create another custom callback to make predictions on the test set and prepare a submission file (5p)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-score (5p)\n",
    "\n",
    "We will use the [F1-score metric](https://en.wikipedia.org/wiki/F1_score) to measure performance. This metric is common in detection problems with unbalanced class problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Definition of F1-score\n",
    "# Tip: use precision_recall_curve() to help you compute precision and recall.\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    \n",
    "    # Compute precision, recall and obtain several detection thresholds\n",
    "    precision, recall, thresholds = None\n",
    "    thresholds = np.append(thresholds, thresholds[-1])\n",
    "    \n",
    "    # Compute F1-score and remove numerical problems\n",
    "    f1 = None\n",
    "    thresholds = thresholds[~np.isnan(f1)]\n",
    "    f1 = f1[~np.isnan(f1)]\n",
    "    \n",
    "    return f1, thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-score curve (5p)\n",
    "\n",
    "Create a function that plots the F1-score with respect to the detection threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_f1_curve(y_true, y_pred, output_path):\n",
    "    \n",
    "    # Compute F1-score\n",
    "    f1, thresholds = None\n",
    "    \n",
    "    # Plot\n",
    "    tag = 'Max %f @ %f' % (np.max(f1), thresholds[np.argmax(f1)])\n",
    "    plt.plot(thresholds, f1, label=tag)\n",
    "    plt.title('F1-score')\n",
    "    plt.ylim(0.0, 1)\n",
    "    plt.xlim(0.0, 1)\n",
    "    plt.grid(b=True, which='both')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Validation Set (10p)\n",
    "\n",
    "After training the model, you will have to select the optimal detection threshold that maximizes the F1-score. You will do this automatically by creating a custom callback that does the following steps at the end of the training:\n",
    "* Load the best model from disk\n",
    "* Load the validation set as a Sequence\n",
    "* Run the model over the validation samples\n",
    "* Get labels from validation set\n",
    "* Compute the F1-score\n",
    "* Saves the F1-score curve plot to disk\n",
    "* Saves the best detection threshold to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FindBestDetectionThreshold(keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, data_dir, csv_dir, result_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.csv_dir = csv_dir\n",
    "        self.result_dir = result_dir\n",
    "        super(FindBestDetectionThreshold, self).__init__()\n",
    "\n",
    "    def on_train_end(self, epoch, logs=None):\n",
    "        \n",
    "        print('Evaluating validation set ...')\n",
    "        \n",
    "        # Load best model\n",
    "        model_path = join(self.result_dir, 'best_model.h5')\n",
    "        best_model = None\n",
    "        \n",
    "        # Load validation set as a sequence\n",
    "        validation_seq = PatchSequence(\n",
    "            input_dir=join(self.data_dir, 'training', '*'), \n",
    "            csv_path=join(self.csv_dir, 'validation_data.csv'), \n",
    "            batch_size=32\n",
    "        )\n",
    "        \n",
    "        # Run the model over samples\n",
    "        y_pred = self.model.predict_generator(None)[:, 1]\n",
    "        \n",
    "        # Get labels\n",
    "        y_true = validation_seq.get_all_labels()\n",
    "        \n",
    "        # Compute F1-score\n",
    "        f1, thresholds = f1_score(y_true, y_pred)\n",
    "        \n",
    "        # Save F1-score curve\n",
    "        plot_f1_curve(y_true, y_pred, join(self.result_dir, 'f1-score_validation.png'))\n",
    "\n",
    "        # Save best detection threshold\n",
    "        th = {'threshold': thresholds[np.argmax(f1)], 'f1': f1[np.argmax(f1)]}\n",
    "        pd.DataFrame(th, index=[0]).to_csv(join(self.result_dir, 'best_threshold.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Test Set (5p)\n",
    "\n",
    "After training the model, you will have to make predictions for every sample in the test set and create a CSV file for the submission to https://ismi-amida13.grand-challenge.org/\n",
    "\n",
    "We will take advantage of Keras callbacks to generate the CSV automatically at the end of the training. You will create a callback that does the following at the end of the training:\n",
    "* Load the best model from disk\n",
    "* Load the test set as a Sequence\n",
    "* Run the model over the test samples\n",
    "* Read best threshold file\n",
    "* Threshold predictions\n",
    "* Store the predictions in a suitable CSV for the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CreateFinalSubmission(keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, data_dir, csv_dir, result_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.csv_dir = csv_dir\n",
    "        self.result_dir = result_dir\n",
    "        super(CreateFinalSubmission, self).__init__()\n",
    "\n",
    "    def on_train_end(self, epoch, logs=None):\n",
    "        \n",
    "        print('Evaluating test set ...')\n",
    "\n",
    "        # Load best model\n",
    "        model_path = join(self.result_dir, 'best_model.h5')\n",
    "        best_model = None\n",
    "        \n",
    "        # Load test set as a sequence\n",
    "        test_seq = PatchSequence(\n",
    "            input_dir=join(self.data_dir, 'test'), \n",
    "            csv_path=join(self.csv_dir, 'test_data.csv'), \n",
    "            batch_size=32\n",
    "        )\n",
    "        \n",
    "        # Run the model over samples\n",
    "        y_pred = self.model.predict_generator(None)[:, 1]\n",
    "        \n",
    "        # Read best threshold\n",
    "        df_th = pd.DataFrame.from_csv(join(self.result_dir, 'best_threshold.csv'))\n",
    "        th = None\n",
    "        \n",
    "        # Threshold predictions\n",
    "        y_pred_bin = None\n",
    "\n",
    "        # Make CSV\n",
    "        df = test_seq.df\n",
    "        df['label'] = y_pred_bin\n",
    "        df['file_id'] = df['file_id'].apply(lambda x: x[:-4])\n",
    "        df = df.loc[:, ['label', 'file_id']]\n",
    "        df.to_csv(join(self.result_dir, 'submission.csv'), index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6. Train a classifier: 10 points\n",
    "\n",
    "You will create a function to train a model and submit the results (10p). The function should follow the next steps:\n",
    "* Define the training generator with augmentation\n",
    "* Define the validation sequence\n",
    "* Build the model\n",
    "* Define simple and advanced callbacks (in particular: evaluation of validation and test sets)\n",
    "* Train the model using fit_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(data_dir, csv_dir, result_dir, training_csv, validation_csv, batch_size=32, epochs=10, augmentation_fn=None, debug=False):\n",
    "    \n",
    "    # Prepare dir\n",
    "    if not exists(result_dir):\n",
    "        os.mkdir(result_dir)\n",
    "    \n",
    "    # Training data\n",
    "    training_data = PatchGenerator(\n",
    "        input_dir=join(data_dir, 'training', '*'), \n",
    "        csv_path=join(csv_dir, training_csv), \n",
    "        batch_size=batch_size,\n",
    "        augmentation_fn=augmentation_fn\n",
    "    )\n",
    "\n",
    "    # Validation data\n",
    "    validation_data = PatchSequence(\n",
    "        input_dir=join(data_dir, 'training', '*'), \n",
    "        csv_path=join(csv_dir, validation_csv), \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "        \n",
    "    # Build model\n",
    "    model = build_model()\n",
    "    model.summary\n",
    "    \n",
    "    # Define callbacks\n",
    "    callbacks_list = [\n",
    "        None\n",
    "        PlotLosses(),\n",
    "        FindBestDetectionThreshold(data_dir=data_dir, csv_dir=csv_dir, result_dir=result_dir),\n",
    "        CreateFinalSubmission(data_dir=data_dir, csv_dir=csv_dir, result_dir=result_dir)\n",
    "    ]\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit_generator(None)\n",
    "    \n",
    "# Train the first model\n",
    "train_model(\n",
    "    data_dir=data_dir, \n",
    "    csv_dir=result_dir,\n",
    "    result_dir=join(result_dir, 'initial'),\n",
    "    training_csv='training_data.csv',\n",
    "    validation_csv='validation_data.csv', \n",
    "    batch_size=32, \n",
    "    epochs=10, \n",
    "    augmentation_fn=basic_augmentation, \n",
    "    debug=True\n",
    ")\n",
    "\n",
    "# Remember to submit the results (manually)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7. Hard negative mining: 10 points\n",
    "\n",
    "By training a CNN to perform a discriminative task, we are effectively **building a decision boundary (an hyper-plane) that is able to separate mitotic from non-mitotic patches**. In particular, each training sample that the classifier sees pushes and changes the shape of this decision boundary so that more and more difficult samples are correctly classified.\n",
    "\n",
    "If you check the negative samples available in the dataset, you will quickly realize that **most of them are very easy to classify**. Typically, they depict tissue that has nothing to do with mitotic figures, therefore, the classifier quickly learns to discriminate those. However, the algorithm will fail in negative patches that resemble mitotic figures (difficult samples). These samples are scarce, therefore, the CNN is not penalized enough for that. For us, this is important since detecting only a few mitotic figures wrong is troublesome (this is common in medical applications). \n",
    "\n",
    "In order to push the performance of our model further, we will include difficult samples more often in the training set. We will define the difficulty of a sample based on the prediction score produced by the initial model. If a training sample is classified with very low probability, then it's an easy sample, otherwise, it's a difficult one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute scores for test set (5p)\n",
    "\n",
    "The first thing you will do is computing a prediction score for each training sample and store it in a Pandas dataframe together with the rest of the sample attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the training set as a sequence\n",
    "training_seq = PatchSequence(\n",
    "    input_dir=join(data_dir, 'training', '*'), \n",
    "    csv_path=join(result_dir, 'training_data.csv'),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Load the initial mode\n",
    "model_path = join(result_dir, 'initial', 'best_model.h5')\n",
    "initial_model = None\n",
    "\n",
    "# Predict\n",
    "y_train_pred = initial_model.predict_generator(None)\n",
    "\n",
    "# Store in dataframe\n",
    "df_training_data = pd.DataFrame.from_csv(join(result_dir, 'training_data.csv'))\n",
    "df_training_data['pred'] = None\n",
    "df_training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample the training set (5p)\n",
    "\n",
    "You will create a new training dataset (CSV file) based on the prediction scores. Sample the same number of samples, with replacement, proportionally to these scores so that difficult negative samples are sampled more often. Make sure you keep the same positive samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select all positive samples\n",
    "df_training_positives = None\n",
    "\n",
    "# Select all negative samples\n",
    "df_training_negatives = None\n",
    "\n",
    "# Resample negative samples proportionally to their scores\n",
    "df_training_hard_negatives = df_training_negatives.sample(None)\n",
    "\n",
    "# Concatenate positives and hard negative samples\n",
    "df_hard_training_data = pd.concat([df_training_positives, df_training_hard_negatives])\n",
    "\n",
    "# Store CSV on disk\n",
    "df_hard_training_data.to_csv(join(result_dir, 'difficult_training_data.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot histograms of scores before and after sampling and make sure there are more difficult samples in the second set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training_data.loc[df_training_data['label'] == 0, :].hist('pred')\n",
    "plt.show()\n",
    "df_hard_training_data.loc[df_hard_training_data['label'] == 0, :].hist('pred')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8. Retrain the Classifier: 10 points\n",
    "\n",
    "You will train the model with the hard dataset and submit the results (10p). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the second model\n",
    "train_model(None)\n",
    "\n",
    "# Remember to submit the results (manually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
